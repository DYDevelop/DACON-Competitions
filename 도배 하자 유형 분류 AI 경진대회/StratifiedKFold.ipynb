{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import datetime as dt\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from joblib import parallel_backend\n",
    "import ttach as tta\n",
    "import timm\n",
    "from timm.models.layers import Conv2dSame\n",
    "from sklearn.metrics import f1_score\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from focal_loss.focal_loss import FocalLoss\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':30,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정\n",
    "time_now = dt.datetime.now()\n",
    "run_id = time_now.strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_list = glob.glob('./train/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['img_path', 'label'])\n",
    "df['img_path'] = all_img_list\n",
    "df['label'] = df['img_path'].apply(lambda x : str(x).split('\\\\')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'].values)\n",
    "\n",
    "assert len(le.classes_) == 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            print(f'Best F1 score from now: {self.best_score}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, transforms=None):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = models.maxvit_t(pretrained=True)\n",
    "        # self.backbone = timm.create_model('densenet201', pretrained=True, num_classes = 1000)\n",
    "        self.classifier = nn.Linear(1000, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "                        cm, classes, runid, epoch, \n",
    "                        f1, normalize=False, \n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(f'{title}-{runid}-{epoch}-{f1:.4f}')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(f'./cms/cm-{runid}.jpg', dpi=400)\n",
    "    plt.clf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixup & Cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix_data(x, y, beta=1):\n",
    "    lam = np.random.beta(beta, beta)\n",
    "    rand_index = torch.randperm(x.size()[0]).cuda()\n",
    "    target_a = y\n",
    "    target_b = y[rand_index]              \n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
    "    \n",
    "    return x, target_a, target_b, lam\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "                            # A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            # A.CenterCrop(p=0.5, height=112, width=112),\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            A.HorizontalFlip(),\n",
    "                            # A.RandomContrast(limit=0.2),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "extra_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            A.HorizontalFlip(),\n",
    "                            A.RandomContrast(limit=0.2),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "test_transform = A.Compose([\n",
    "                            A.Resize(CFG['IMG_SIZE'],CFG['IMG_SIZE']),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 Fold 별 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skf = sklearn.model_selection.StratifiedKFold(n_splits=20, shuffle=False)\n",
    "skf = sklearn.model_selection.StratifiedKFold(n_splits=10, shuffle=False)\n",
    "t = df.label\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(np.zeros(len(t)), t)):\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "    data_train = df.loc[train_index]\n",
    "    data_validation = df.loc[test_index]\n",
    "\n",
    "    class_counts = data_train['label'].value_counts(sort=False).to_dict()\n",
    "    num_samples = sum(class_counts.values())\n",
    "    print(f'cls_cnts: {len(class_counts)}\\nnum_samples:{num_samples}')\n",
    "    labels = data_train['label'].to_list()\n",
    "\n",
    "    # weight 제작, 전체 학습 데이터 수를 해당 클래스의 데이터 수로 나누어 줌\n",
    "    class_weights = {l:round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
    "\n",
    "    # class 별 weight를 전체 trainset에 대응시켜 sampler에 넣어줌\n",
    "    weights = [class_weights[labels[i]] for i in range(int(num_samples))] \n",
    "    sampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
    "    \n",
    "    # batch_size=16\n",
    "    train_dataset = CustomDataset(df.loc[train_index]['img_path'].values, df.loc[train_index]['label'].values, train_transform)\n",
    "    # extra_dataset = CustomDataset(df.loc[df.loc[train_index]['label'] == '9']['img_path'].values, df.loc[df.loc[train_index]['label'] == '9']['label'].values, extra_transform)\n",
    "    validation_dataset = CustomDataset(df.loc[test_index]['img_path'].values, df.loc[test_index]['label'].values, test_transform)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=32,\n",
    "        sampler=sampler,  # trainset에 sampler를 설정해줌\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "        )\n",
    "    validation_loader = DataLoader(\n",
    "        validation_dataset, \n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0)\n",
    "\n",
    "    test = pd.read_csv('./test.csv')\n",
    "    test_dataset = CustomDataset(test['img_path'].values, None, test_transform)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=32,\n",
    "        shuffle=False, \n",
    "        num_workers=0,\n",
    "        pin_memory=True)\n",
    "\n",
    "    dataloaders = {\n",
    "        'train': train_loader,\n",
    "        'val': validation_loader,\n",
    "        'test': test_loader\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {\n",
    "        'train': len(train_dataset),\n",
    "        'val': len(validation_dataset),\n",
    "        'test': len(test_dataset)\n",
    "    }\n",
    "\n",
    "    # timm에서 모델을 가져옴\n",
    "    device =  torch.device(\"cuda\")\n",
    "    # model = timm.create_model('tf_efficientnet_b7_ns', pretrained=True, num_classes=19)\n",
    "    model = BaseModel()\n",
    "    model.to(device)\n",
    "\n",
    "    epochs = 300  # 보통 30~40 epoch에서 멈춥니다.\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    # with weights \n",
    "    # The weights parameter is similar to the alpha value mentioned in the paper\n",
    "    # weights_for_loss = torch.FloatTensor(list(class_weights.values())).to(device)\n",
    "    # m = torch.nn.Softmax(dim=-1)\n",
    "    # criterion = FocalLoss(gamma=2.0, weights=weights_for_loss)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    # scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-8)\n",
    "\n",
    "    os.makedirs(f'./runs/{run_id}', exist_ok=True)\n",
    "    os.makedirs(f'./cms/', exist_ok=True)\n",
    "    \n",
    "    since = time.time()\n",
    "    best_f1 = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    fold_run_id = f'{run_id}_fold{str(fold)}'\n",
    " \n",
    "    # 학습\n",
    "    for epoch in range(epochs):\n",
    "        print('-'*50)\n",
    "        print(f'Fold: {fold}')\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.0\n",
    "            cm_preds = []\n",
    "            cm_labels = []\n",
    "            model_preds = []\n",
    "            model_labels = []\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            for x, y in tqdm(iter(dataloaders[phase])):\n",
    "                x = x.to(device)\n",
    "                y = y.type(torch.LongTensor).to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                r, flag = np.random.rand(1), 1\n",
    "                \n",
    "                if phase == 'train': # CutMix\n",
    "                    inputs, targets_a, targets_b, lam = cutmix_data(x, y) \n",
    "                    inputs, targets_a, targets_b = map(Variable, (inputs,\n",
    "                                                            targets_a, targets_b))\n",
    "                \n",
    "                # if phase == 'train': # MixUp\n",
    "                #     inputs, targets_a, targets_b, lam = mixup_data(x, y)\n",
    "                #     inputs, targets_a, targets_b = map(Variable, (inputs,\n",
    "                #                                             targets_a, targets_b))\n",
    "                \n",
    "                elif phase == 'val': flag = 0 # Original\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with torch.cuda.amp.autocast(enabled=True):\n",
    "                        y_hat = model(x)\n",
    "                        if flag == 0: loss = criterion(y_hat, y)\n",
    "                        else: loss = mixup_criterion(criterion, y_hat, targets_a, targets_b, lam)\n",
    "                    _, preds = torch.max(y_hat, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                running_loss += loss.item() * x.size(0)\n",
    "                \n",
    "                model_labels += y.detach().cpu().numpy().tolist()\n",
    "                model_preds += preds.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_f1 = f1_score(\n",
    "                        model_labels, \n",
    "                        model_preds, \n",
    "                        average='weighted')\n",
    "            print(f'[{phase}] Loss: {epoch_loss:.4f} Weighted F1: {epoch_f1:.4f}')\n",
    "            \n",
    "            if phase == 'val' and scheduler != None:\n",
    "                scheduler.step(epoch_f1)\n",
    "\n",
    "            # 체크포인트 저장\n",
    "            if phase == 'val':\n",
    "                if epoch_f1 > best_f1:\n",
    "                    best_f1 = epoch_f1\n",
    "                    torch.save(model, f'./runs/{run_id}/best_model_fold{fold}.pt')\n",
    "                    confusion_mtx = confusion_matrix(model_labels, model_preds)\n",
    "                    plot_confusion_matrix(confusion_mtx, classes=class_counts.keys(), runid=fold_run_id, epoch=epoch, f1=best_f1)\n",
    "                else:\n",
    "                    # torch.save(model, f'./runs/{run_id}/{epoch}-val_loss{epoch_loss}-val_f1{epoch_f1}.pt')\n",
    "                    pass\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        stop = early_stopping(epoch_f1)\n",
    "        if stop:\n",
    "            print(\"called\")   \n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val_F1: {:4f}'.format(best_f1))\n",
    "\n",
    "    # 해당 fold의 checkpoint를 불러와 test\n",
    "    device =  torch.device(\"cuda\")\n",
    "    checkpoint = f'./runs/{run_id}/best_model_fold{fold}.pt'\n",
    "    print(f'CHECKPOINT LOADED: {checkpoint}')\n",
    "    model = torch.load(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x in tqdm(iter(dataloaders['test'])):\n",
    "                x = x.to(device)\n",
    "                batch_pred = model(x)\n",
    "                _, pred = torch.max(batch_pred, 1)\n",
    "                pred = pred.detach().cpu().numpy().tolist()\n",
    "                test_preds.extend(pred)\n",
    "\n",
    "    # trainset에 fit_trainsform 되어있는 LabelEncoder로 inverse transform 해줌\n",
    "    test_preds = le.inverse_transform(test_preds)\n",
    "\n",
    "    submit = pd.read_csv('./sample_submission.csv')\n",
    "    submit['label'] = test_preds\n",
    "    \n",
    "    submit.loc[submit['label'] == '0', 'label'] = '가구수정'\n",
    "    submit.loc[submit['label'] == '1', 'label'] = '걸레받이수정'\n",
    "    submit.loc[submit['label'] == '2', 'label'] = '곰팡이'\n",
    "    submit.loc[submit['label'] == '3', 'label'] = '꼬임'\n",
    "    submit.loc[submit['label'] == '4', 'label'] = '녹오염'\n",
    "    submit.loc[submit['label'] == '5', 'label'] = '들뜸'\n",
    "    submit.loc[submit['label'] == '6', 'label'] = '면불량'\n",
    "    submit.loc[submit['label'] == '7', 'label'] = '몰딩수정'\n",
    "    submit.loc[submit['label'] == '8', 'label'] = '반점'\n",
    "    submit.loc[submit['label'] == '9', 'label'] = '석고수정'\n",
    "    submit.loc[submit['label'] == '10', 'label'] = '오염'\n",
    "    submit.loc[submit['label'] == '11', 'label'] = '오타공'\n",
    "    submit.loc[submit['label'] == '12', 'label'] = '울음'\n",
    "    submit.loc[submit['label'] == '13', 'label'] = '이음부불량'\n",
    "    submit.loc[submit['label'] == '14', 'label'] = '창틀,문틀수정'\n",
    "    submit.loc[submit['label'] == '15', 'label'] = '터짐'\n",
    "    submit.loc[submit['label'] == '16', 'label'] = '틈새과다'\n",
    "    submit.loc[submit['label'] == '17', 'label'] = '피스'\n",
    "    submit.loc[submit['label'] == '18', 'label'] = '훼손'\n",
    "\n",
    "    os.makedirs('./output/', exist_ok=True)\n",
    "    submit.to_csv(f'./output/{run_id}_fold{fold}.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folds 투표 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['훼손', '오염', '오염', '몰딩수정', '오염', '오타공', '오염', '오염', '오염', '오타공', '몰딩수정', '오타공', '오염', '오염', '훼손', '훼손', '훼손', '걸레받이수정', '오염', '오염']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# 경로 수정(run_id*.csv)\n",
    "csvs = glob('./output/20230502015644*.csv')\n",
    "# csvs2 = glob('./output/20221030183609*.csv')\n",
    "# csvs.extend(csvs2)\n",
    "print(len(csvs))\n",
    "\n",
    "preds = []\n",
    "for csv in csvs:\n",
    "    f = pd.read_csv(csv)\n",
    "    label = f['label'].tolist()\n",
    "    preds.append(label)\n",
    "\n",
    "# stacked = pd.read_csv('./stack/Stacked_3_Classes.csv')\n",
    "# select = ['곰팡이', '석고수정', '오염']\n",
    "out = []\n",
    "cols = list(zip(*preds))\n",
    "for i, c in enumerate(cols):\n",
    "    most = Counter(c).most_common()[0][0]\n",
    "    # if most in select: most = stacked.loc[i]['label']\n",
    "    out.append(most)\n",
    "\n",
    "print(out[:20])\n",
    "ss = pd.read_csv('./sample_submission.csv')\n",
    "ss['label'] = out\n",
    "ss.to_csv('vote1234.csv', index=False)  # 구분 가능하게 경로 수정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
